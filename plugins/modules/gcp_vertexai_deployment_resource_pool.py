#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2017-2026 Google
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
# ----------------------------------------------------------------------------
#

from __future__ import absolute_import, division, print_function

__metaclass__ = type

################################################################################
# Documentation
################################################################################

ANSIBLE_METADATA = {
    "metadata_version": "1.1",
    "status": ["preview"],
    "supported_by": "community",
}

DOCUMENTATION = r"""
---
author:
  - Google Inc. (@googlecloudplatform)
description:
  - '''DeploymentResourcePool can be shared by multiple deployed models,'
  - whose underlying specification consists of dedicated resources.'
extends_documentation_fragment:
  - google.cloud.gcp
module: gcp_vertexai_deployment_resource_pool
notes:
  - 'API Reference: U(https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.deploymentResourcePools)'
options:
  dedicated_resources:
    description:
      - The underlying dedicated resources that the deployment resource pool uses.
    suboptions:
      autoscaling_metric_specs:
        description:
          - A list of the metric specifications that overrides a resource utilization metric.
        elements: dict
        suboptions:
          metric_name:
            description:
              - The resource metric name.
              - >-
                Supported metrics: For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` *

                `aiplatform.googleapis.com/prediction/online/cpu/utilization`.
            required: true
            type: str
          target:
            description:
              - >-
                The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain

                percentage, the machine replicas change.
              - The default value is 60 (representing 60%) if not provided.
            type: int
        type: list
      machine_spec:
        description:
          - The specification of a single machine used by the prediction.
        required: true
        suboptions:
          accelerator_count:
            description:
              - The number of accelerators to attach to the machine.
            type: int
          accelerator_type:
            description:
              - The type of accelerator(s) that may be attached to the machine as per accelerator_count.
              - See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
            type: str
          machine_type:
            description:
              - The type of the machine.
              - >-
                See the [list of machine types supported for

                prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
            type: str
        type: dict
      max_replica_count:
        description:
          - The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases.
          - >-
            If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many

            replicas is guaranteed (barring service outages).
          - If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped.
          - If this value is not provided, will use min_replica_count as the default value.
          - The value of this field impacts the charge against Vertex CPU and GPU quotas.
          - >-
            Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of

            GPUs per replica in the selected machine type).
        type: int
      min_replica_count:
        description:
          - The minimum number of machine replicas this DeployedModel will be always deployed on.
          - This value must be greater than or equal to 1.
          - >-
            If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these

            extra replicas may be freed.
        required: true
        type: int
    type: dict
  name:
    description:
      - The resource name of deployment resource pool.
      - The maximum length is 63 characters, and valid characters are `/^[a-z]([a-z0-9-]{0,61}[a-z0-9])?$/`.
    required: true
    type: str
  region:
    description:
      - The region of deployment resource pool.
      - eg us-central1.
    type: str
  state:
    choices:
      - present
      - absent
    default: present
    description:
      - Whether the resource should exist in GCP.
    type: str
requirements:
  - python >= 3.8
  - requests >= 2.18.4
  - google-auth >= 2.25.1
short_description: Creates a GCP VertexAI.DeploymentResourcePool resource
"""

EXAMPLES = r"""
- name: Create Deployment Resource Pool
  google.cloud.gcp_vertexai_deployment_resource_pool:
    state: present
    name: my_deployment_resource_pool
    dedicated_resources:
      min_replica_count: 1
      max_replica_count: 3
      machine_spec:
        machine_type: n1-standard-4
        accelerator_type: NVIDIA_TESLA_P4
        accelerator_count: 1
    region: us-central1
    project: "{{ gcp_project }}"
    auth_kind: "{{ gcp_cred_kind }}"
    service_account_file: "{{ gcp_cred_file }}"
"""

RETURN = r"""
changed:
  description: Whether the resource was changed.
  returned: always
  type: bool
createTime:
  description:
    - A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.
  returned: success
  type: str
state:
  description: The current state of the resource.
  returned: always
  type: str
"""

################################################################################
# Imports
################################################################################

from ansible_collections.google.cloud.plugins.module_utils import gcp_utils as gcp
import types

# BEGIN Custom imports
import copy

# END Custom imports


def build_link(module_params, uri):
    params = module_params.copy()

    return ("https://{region}-aiplatform.googleapis.com/v1/" + uri).format(**params)


class DedicatedResources(gcp.Resource):
    def _request(self):
        return {
            "autoscalingMetricSpecs": [
                DedicatedResourcesAutoscalingMetricSpec(item).to_request()
                for item in (self.request.get("autoscaling_metric_specs") or [])
            ],
            "machineSpec": gcp.remove_empties(
                DedicatedResourcesMachineSpec(self.request.get("machine_spec", {})).to_request()
            ),  # remove empty values
            "maxReplicaCount": self.request.get("max_replica_count"),
            "minReplicaCount": self.request.get("min_replica_count"),
        }

    def _response(self):
        return {
            "autoscalingMetricSpecs": [
                DedicatedResourcesAutoscalingMetricSpec().from_response(item)
                for item in (self.response.get("autoscalingMetricSpecs") or [])
            ],
            "machineSpec": DedicatedResourcesMachineSpec().from_response(self.response.get("machineSpec", {})),
            "maxReplicaCount": self.response.get("maxReplicaCount"),
            "minReplicaCount": self.response.get("minReplicaCount"),
        }


class DedicatedResourcesAutoscalingMetricSpec(gcp.Resource):
    def _request(self):
        return {
            "metricName": self.request.get("metric_name"),
            "target": self.request.get("target"),
        }

    def _response(self):
        return {
            "metricName": self.response.get("metricName"),
            "target": self.response.get("target"),
        }


class DedicatedResourcesMachineSpec(gcp.Resource):
    def _request(self):
        return {
            "acceleratorCount": self.request.get("accelerator_count"),
            "acceleratorType": self.request.get("accelerator_type"),
            "machineType": self.request.get("machine_type"),
        }

    def _response(self):
        return {
            "acceleratorCount": self.response.get("acceleratorCount"),
            "acceleratorType": self.response.get("acceleratorType"),
            "machineType": self.response.get("machineType"),
        }


class VertexAI(gcp.Resource):
    def _request(self):
        return {
            "dedicatedResources": gcp.remove_empties(
                DedicatedResources(self.request.get("dedicated_resources", {})).to_request()
            ),  # remove empty values
            "name": self.request.get("name"),
        }

    def _response(self):
        return {
            "createTime": self.response.get("createTime"),
            "dedicatedResources": DedicatedResources().from_response(self.response.get("dedicatedResources", {})),
            "name": self.response.get("name"),
        }


################################################################################
# Main
################################################################################


def encode(self, obj):
    """
    This is a function bound to the main resource object. Its input is the object returned from to_request()
    and it mutates it before it is sent to the API.
    """
    # --------- BEGIN custom encoder code ---------
    self.debug(func="encoder", obj=obj)
    action = getattr(self, "_action", "read")
    r = {}
    if action == "create":
        r = {
            "deploymentResourcePool": copy.deepcopy(obj),
            "deploymentResourcePoolId": obj["name"],
        }
    elif action == "read":
        r = copy.deepcopy(obj)
    elif action == "update":
        r = copy.deepcopy(obj)
    else:
        pass

    return r
    # --------- END custom encoder code ---------


def decode(self, obj):
    """
    This is a function bound to the main resource object. Its input is the object returned from from_response()
    and it mutates it before it is returned to the module caller.
    """
    return obj


def main():
    """Main function"""

    module = gcp.Module(
        argument_spec=dict(
            name=dict(
                type="str",
                required=True,
            ),
            state=dict(
                type="str",
                default="present",
                choices=["present", "absent"],
            ),
            dedicated_resources=dict(
                type="dict",
                options=dict(
                    autoscaling_metric_specs=dict(
                        type="list",
                        elements="dict",
                        options=dict(
                            metric_name=dict(
                                type="str",
                                required=True,
                            ),
                            target=dict(
                                type="int",
                            ),
                        ),
                    ),
                    machine_spec=dict(
                        type="dict",
                        required=True,
                        options=dict(
                            accelerator_count=dict(
                                type="int",
                            ),
                            accelerator_type=dict(
                                type="str",
                            ),
                            machine_type=dict(
                                type="str",
                            ),
                        ),
                    ),
                    max_replica_count=dict(
                        type="int",
                    ),
                    min_replica_count=dict(
                        type="int",
                        required=True,
                    ),
                ),
            ),
            region=dict(
                type="str",
            ),
        )
    )

    if not module.params["scopes"]:
        module.params["scopes"] = ["https://www.googleapis.com/auth/cloud-platform"]

    state = module.params["state"]
    changed = False
    op_configs = gcp.ResourceOpConfigs(
        {
            "base_url": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{region}/deploymentResourcePools",
                    "async_uri": "",
                    "verb": "GET",
                    "timeout_minutes": 0,
                }
            ),
            "create": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{region}/deploymentResourcePools",
                    "async_uri": "{op_id}",
                    "verb": "POST",
                    "timeout_minutes": 20,
                }
            ),
            "delete": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{region}/deploymentResourcePools/{name}",
                    "async_uri": "{op_id}",
                    "verb": "DELETE",
                    "timeout_minutes": 20,
                }
            ),
            "read": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{region}/deploymentResourcePools/{name}",
                    "async_uri": "",
                    "verb": "GET",
                    "timeout_minutes": 0,
                }
            ),
            "update": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{region}/deploymentResourcePools/{name}",
                    "async_uri": "",
                    "verb": "PATCH",
                    "timeout_minutes": 20,
                }
            ),
        }
    )

    params = gcp.remove_nones(module.params)
    resource = VertexAI(params, module=module, product="VertexAI", kind="vertexai#deploymentResourcePool")
    read_uri = op_configs.read.uri

    resource._state = state  # store the state in the resource object
    # Bind the encode and decode functions to the resource object
    resource.encode_func = types.MethodType(encode, resource)
    resource.decode_func = types.MethodType(decode, resource)

    custom_diff = None  # Set this variable if you want to implement custom diff logic

    read_url = build_link(params, read_uri)
    existing_obj = resource.get(read_url, allow_not_found=True) or {}
    new_obj = {}
    gcp.debug(module, existing=existing_obj, post=False)

    if custom_diff is not None:
        is_different = custom_diff
    else:
        is_different = resource.diff(gcp.remove_empties(existing_obj))
    gcp.debug(
        module,
        request=gcp.remove_empties(resource.to_request()),
        existing=existing_obj,
        post=True,
        is_different=is_different,
    )

    if gcp.empty(existing_obj):
        if state == "present":
            create_uri = op_configs.create.uri
            create_async_uri = op_configs.create.async_uri
            try:
                # --------- BEGIN create code ---------
                # --------- BEGIN pre-create custom code ---------
                resource._action = "create"

                # --------- END pre-create custom code ---------
                is_async = create_async_uri != ""
                create_link = build_link(params, create_uri)
                create_retries = op_configs.create.timeout
                create_func = getattr(resource, op_configs.create.verb)
                async_create_func = getattr(resource, op_configs.create.verb + "_async")
                async_create_link = build_link(params, "") + create_async_uri
                gcp.debug(
                    module,
                    msg="Creating resource",
                    create_link=create_link,
                    async_create_link=async_create_link,
                    is_async=is_async,
                )

                if is_async:
                    new_obj = async_create_func(create_link, async_link=async_create_link, retries=create_retries)
                else:
                    new_obj = create_func(create_link)
                gcp.debug(module, new=new_obj, action="create", post=False)
                gcp.debug(module, new=new_obj, action="create", post=True)
                # --------- END create code ---------
            except Exception as e:
                module.fail_json(msg=str(e))

            changed = True
        else:
            pass  # nothing to do
    else:
        if state == "absent":
            delete_uri = op_configs.delete.uri
            delete_async_uri = op_configs.delete.async_uri
            try:
                # --------- BEGIN delete code ---------
                # --------- BEGIN pre-delete custom code ---------
                resource._action = "delete"

                # --------- END pre-delete custom code ---------
                is_async = delete_async_uri != ""
                delete_link = build_link(params, delete_uri)
                delete_retries = op_configs.delete.timeout
                delete_func = getattr(resource, op_configs.delete.verb)
                async_delete_func = getattr(resource, op_configs.delete.verb + "_async")
                async_delete_link = build_link(params, "") + delete_async_uri
                gcp.debug(
                    module,
                    msg="Destroying resource",
                    delete_link=delete_link,
                    async_delete_link=async_delete_link,
                    is_async=is_async,
                )
                if is_async:
                    new_obj = async_delete_func(delete_link, async_link=async_delete_link, retries=delete_retries)
                else:
                    new_obj = delete_func(delete_link)
                # --------- END delete code ---------
            except Exception as e:
                module.fail_json(msg=str(e))

            changed = True
        else:
            if is_different:
                update_uri = op_configs.update.uri
                update_async_uri = op_configs.update.async_uri
                try:
                    # --------- BEGIN update code ---------
                    # --------- BEGIN pre-update custom code ---------
                    resource._action = "update"
                    update_uri += "?updateMask=" + ",".join(resource.dot_fields())

                    # --------- END pre-update custom code ---------
                    is_async = update_async_uri != ""
                    update_link = build_link(params, update_uri)
                    update_retries = op_configs.update.timeout
                    update_func = getattr(resource, op_configs.update.verb)
                    async_update_func = getattr(resource, op_configs.update.verb + "_async")
                    async_update_link = build_link(params, "") + update_async_uri
                    gcp.debug(
                        module,
                        msg="Updating resource",
                        update_link=update_link,
                        async_update_link=async_update_link,
                        is_async=is_async,
                    )
                    if is_async:
                        new_obj = async_update_func(update_link, async_link=async_update_link, retries=update_retries)
                    else:
                        new_obj = update_func(update_link)
                    gcp.debug(module, new=new_obj, action="update", post=False)
                    gcp.debug(module, new=new_obj, action="update", post=True)
                    # --------- END update code ---------
                except Exception as e:
                    module.fail_json(msg=str(e))

                changed = True
            else:
                new_obj = existing_obj

    new_obj.update({"changed": changed})
    module.exit_json(**new_obj)


if __name__ == "__main__":
    main()
