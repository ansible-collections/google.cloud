#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2017-2026 Google
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
# ----------------------------------------------------------------------------
#

from __future__ import absolute_import, division, print_function

__metaclass__ = type

################################################################################
# Documentation
################################################################################

ANSIBLE_METADATA = {
    "metadata_version": "1.1",
    "status": ["preview"],
    "supported_by": "community",
}

DOCUMENTATION = r"""
---
author:
  - Google Inc. (@googlecloudplatform)
description:
  - Models are deployed into it, and afterwards Endpoint is called to obtain predictions and explanations.
extends_documentation_fragment:
  - google.cloud.gcp
module: gcp_vertexai_endpoint
notes:
  - 'API Reference: U(https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints)'
  - 'Official Documentation Guide: U(https://cloud.google.com/vertex-ai/docs)'
options:
  dedicated_endpoint_enabled:
    description:
      - If true, the endpoint will be exposed through a dedicated DNS [Endpoint.dedicated_endpoint_dns].
      - Your request to the dedicated DNS will be isolated from other users' traffic and will have better performance and reliability.
      - 'Note: Once you enabled dedicated endpoint, you won''t be able to send request to the shared DNS {region}-aiplatform.googleapis.com.'
      - The limitation will be removed soon.
    type: bool
  description:
    description:
      - The description of the Endpoint.
    type: str
  display_name:
    description:
      - The display name of the Endpoint.
      - The name can be up to 128 characters long and can consist of any UTF-8 characters.
    required: true
    type: str
  encryption_spec:
    description:
      - Customer-managed encryption key spec for an Endpoint.
      - If set, this Endpoint and all sub-resources of this Endpoint will be secured by this key.
    suboptions:
      kms_key_name:
        description:
          - The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
          - 'Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`.'
          - The key needs to be in the same region as where the compute resource is created.
        required: true
        type: str
    type: dict
  labels:
    description:
      - The labels with user-defined metadata to organize your Endpoints.
      - >-
        Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters,

        underscores and dashes.
      - International characters are allowed.
      - See https://goo.gl/xmQnxf for more information and examples of labels.
    type: dict
  location:
    description:
      - The location for the resource.
    required: true
    type: str
  name:
    description:
      - The resource name of the Endpoint.
      - The name must be numeric with no leading zeros and can be at most 10 digits.
    required: true
    type: str
  network:
    description:
      - >-
        The full name of the Google Compute Engine [network](https://cloud.google.com//compute/docs/networks-and-firewalls#networks) to which the

        Endpoint should be peered.
      - Private services access must already be configured for the network.
      - If left unspecified, the Endpoint is not peered with any network.
      - Only one of the fields, network or enable_private_service_connect, can be set.
      - '[Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert): `projects/{project}/global/networks/{network}`.'
      - Where `{project}` is a project number, as in `12345`, and `{network}` is network name.
      - Only one of the fields, `network` or `privateServiceConnectConfig`, can be set.
    type: str
  predict_request_response_logging_config:
    description:
      - Configures the request-response logging for online prediction.
    suboptions:
      bigquery_destination:
        description:
          - BigQuery table for logging.
          - >-
            If only given a project, a new dataset will be created with name `logging_<endpoint-display-name>_<endpoint-id>` where will be made

            BigQuery-dataset-name compatible (e.g.
          - most special characters will become underscores).
          - If no table name is given, a new table will be created with name `request_response_logging`.
        suboptions:
          output_uri:
            description:
              - BigQuery URI to a project or table, up to 2000 characters long.
              - When only the project is specified, the Dataset and Table is created.
              - When the full table reference is specified, the Dataset must exist and table must not exist.
              - 'Accepted forms: - BigQuery path.'
              - 'For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.'
            type: str
        type: dict
      enabled:
        description:
          - If logging is enabled or not.
        type: bool
      sampling_rate:
        description:
          - Percentage of requests to be logged, expressed as a fraction in range(0,1].
        type: str
    type: dict
  private_service_connect_config:
    description:
      - Configuration for private service connect.
      - '`network` and `privateServiceConnectConfig` are mutually exclusive.'
    suboptions:
      enable_private_service_connect:
        description:
          - If true, expose the IndexEndpoint via private service connect.
        required: true
        type: bool
      enable_secure_private_service_connect:
        description:
          - If set to true, enable secure private service connect with IAM authorization.
          - Otherwise, private service connect will be done without authorization.
          - Note latency will be slightly increased if authorization is enabled.
        type: bool
      project_allowlist:
        description:
          - A list of Projects from which the forwarding rule will target the service attachment.
        elements: str
        type: list
      psc_automation_configs:
        description:
          - List of projects and networks where the PSC endpoints will be created.
          - This field is used by Online Inference(Prediction) only.
        elements: dict
        suboptions:
          error_message:
            description:
              - Error message if the PSC service automation failed.
            type: str
          forwarding_rule:
            description:
              - Forwarding rule created by the PSC service automation.
            type: str
          ip_address:
            description:
              - IP address rule created by the PSC service automation.
            type: str
          network:
            description:
              - The full name of the Google Compute Engine [network](https://cloud.google.com/compute/docs/networks-and-firewalls#networks).
              - '[Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/get): projects/{project}/global/networks/{network}.'
            required: true
            type: str
          project_id:
            description:
              - Project id used to create forwarding rule.
            required: true
            type: str
          state:
            choices:
              - PSC_AUTOMATION_STATE_FAILED
              - PSC_AUTOMATION_STATE_SUCCESSFUL
            description:
              - The state of the PSC service automation.
            type: str
        type: list
    type: dict
  region:
    description:
      - The region for the resource.
    type: str
  state:
    choices:
      - present
      - absent
    default: present
    description:
      - Whether the resource should exist in GCP.
    type: str
  traffic_split:
    description:
      - A map from a DeployedModel's id to the percentage of this Endpoint's traffic that should be forwarded to that DeployedModel.
      - If a DeployedModel's id is not listed in this map, then it receives no traffic.
      - The traffic percentage values must add up to 100, or map must be empty if the Endpoint is to not accept any traffic at a moment.
      - >-
        See the `deployModel` [example](https://cloud.google.com/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint) and

        [documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/deployModel) for more

        information.
      - ~> **Note:** To set the map to empty, set `"{}"`, apply, and then remove the field from your config.
    type: str
requirements:
  - python >= 3.8
  - requests >= 2.18.4
  - google-auth >= 2.25.1
short_description: Creates a GCP VertexAI.Endpoint resource
"""

EXAMPLES = r"""
- name: Create Endpoint
  google.cloud.gcp_vertexai_endpoint:
    name: "{{ resource_name }}"
    state: present
    display_name: "{{ resource_name }}"
    # network: "projects/{{ gcp_project_number }}/global/networks/{{ mynet }}"  # Network must be peered
    location: us-central1
    region: us-central1
    project: "{{ gcp_project }}"
    auth_kind: "{{ gcp_cred_kind }}"
    service_account_file: "{{ gcp_cred_file }}"
  register: _myep

- name: Print Endpoint
  ansible.builtin.debug:
    var: _myep

- name: Delete Endpoint
  google.cloud.gcp_vertexai_endpoint:
    name: "{{ resource_name }}"
    state: absent
    display_name: "{{ resource_name }}"
    # network: "projects/{{ gcp_project_number }}/global/networks/{{ mynet }}"
    location: us-central1
    region: us-central1
    project: "{{ gcp_project }}"
    auth_kind: "{{ gcp_cred_kind }}"
    service_account_file: "{{ gcp_cred_file }}"
"""

RETURN = r"""
changed:
  description: Whether the resource was changed.
  returned: always
  type: bool
createTime:
  description:
    - Output only.
    - Timestamp when this Endpoint was created.
  returned: success
  type: str
dedicatedEndpointDns:
  description:
    - Output only.
    - DNS of the dedicated endpoint.
    - Will only be populated if dedicatedEndpointEnabled is true.
    - 'Format: `https://{endpointId}.{region}-{projectNumber}.prediction.vertexai.goog`.'
  returned: success
  type: str
deployedModels:
  contains:
    automaticResources:
      contains:
        maxReplicaCount:
          description:
            - The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases.
            - >-
              If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many

              replicas is guaranteed (barring service outages).
            - If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped.
            - >-
              If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale

              beyond certain replica number.
          returned: success
          type: int
        minReplicaCount:
          description:
            - The minimum number of replicas this DeployedModel will be always deployed on.
            - >-
              If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some

              of these extra replicas may be freed.
            - If the requested value is too large, the deployment will error.
          returned: success
          type: int
      description:
        - A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
      returned: success
      type: dict
    createTime:
      description:
        - Output only.
        - Timestamp when the DeployedModel was created.
      returned: success
      type: str
    dedicatedResources:
      contains:
        autoscalingMetricSpecs:
          contains:
            metricName:
              description:
                - The resource metric name.
                - >-
                  Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` *

                  `aiplatform.googleapis.com/prediction/online/cpu/utilization`.
              returned: success
              type: str
            target:
              description:
                - >-
                  The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain

                  percentage, the machine replicas change.
                - The default value is 60 (representing 60%) if not provided.
              returned: success
              type: int
          description:
            - >-
              The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value

              (default to 60 if not set).
            - At most one entry is allowed per metric.
            - >-
              If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and

              scale up when either metrics exceeds its target value while scale down if both metrics are under their target value.
            - The default target value is 60 for both metrics.
            - >-
              If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not

              explicitly set.
            - >-
              For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set

              autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to

              `80`.
          elements: dict
          returned: success
          type: list
        machineSpec:
          contains:
            acceleratorCount:
              description:
                - The number of accelerators to attach to the machine.
              returned: success
              type: int
            acceleratorType:
              description:
                - The type of accelerator(s) that may be attached to the machine as per accelerator_count.
                - See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
              returned: success
              type: str
            machineType:
              description:
                - The type of the machine.
                - >-
                  See the [list of machine types supported for

                  prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported

                  for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
                - For DeployedModel this field is optional, and the default value is `n1-standard-2`.
                - For BatchPredictionJob or as part of WorkerPoolSpec this field is required.
                - 'TODO: Try to better unify the required vs optional.'
              returned: success
              type: str
          description:
            - The specification of a single machine used by the prediction.
          returned: success
          type: dict
        maxReplicaCount:
          description:
            - The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases.
            - >-
              If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many

              replicas is guaranteed (barring service outages).
            - If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped.
            - If this value is not provided, will use min_replica_count as the default value.
            - The value of this field impacts the charge against Vertex CPU and GPU quotas.
            - >-
              Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of

              GPUs per replica in the selected machine type).
          returned: success
          type: int
        minReplicaCount:
          description:
            - The minimum number of machine replicas this DeployedModel will be always deployed on.
            - This value must be greater than or equal to 1.
            - >-
              If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these

              extra replicas may be freed.
          returned: success
          type: int
      description:
        - A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
      returned: success
      type: dict
    displayName:
      description:
        - The display name of the DeployedModel.
        - If not provided upon creation, the Model's display_name is used.
      returned: success
      type: str
    enableAccessLogging:
      description:
        - These logs are like standard server access logs, containing information like timestamp and latency for each prediction request.
        - >-
          Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate

          (QPS).
        - Estimate your costs before enabling this option.
      returned: success
      type: bool
    enableContainerLogging:
      description:
        - If true, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Stackdriver Logging.
        - Only supported for custom-trained Models and AutoML Tabular Models.
      returned: success
      type: bool
    id:
      description:
        - The ID of the DeployedModel.
        - If not provided upon deployment, Vertex AI will generate a value for this ID.
        - This value should be 1-10 characters, and valid characters are /[0-9]/.
      returned: success
      type: str
    model:
      description:
        - The name of the Model that this is the deployment of.
        - Note that the Model may be in a different location than the DeployedModel's Endpoint.
      returned: success
      type: str
    modelVersionId:
      description:
        - Output only.
        - The version ID of the model that is deployed.
      returned: success
      type: str
    privateEndpoints:
      contains:
        explainHttpUri:
          description:
            - Output only.
            - Http(s) path to send explain requests.
          returned: success
          type: str
        healthHttpUri:
          description:
            - Output only.
            - Http(s) path to send health check requests.
          returned: success
          type: str
        predictHttpUri:
          description:
            - Output only.
            - Http(s) path to send prediction requests.
          returned: success
          type: str
        serviceAttachment:
          description:
            - Output only.
            - The name of the service attachment resource.
            - Populated if private service connect is enabled.
          returned: success
          type: str
      description:
        - Output only.
        - >-
          Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private

          services access.
        - This field is populated if network is configured.
      returned: success
      type: dict
    serviceAccount:
      description:
        - The service account that the DeployedModel's container runs as.
        - Specify the email address of the service account.
        - If this service account is not specified, the container runs as a service account that doesn't have access to the resource project.
        - Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
      returned: success
      type: str
    sharedResources:
      description:
        - The resource name of the shared DeploymentResourcePool to deploy on.
        - 'Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}.'
      returned: success
      type: str
  description:
    - Output only.
    - The models deployed in this Endpoint.
    - To add or remove DeployedModels use EndpointService.DeployModel and EndpointService.UndeployModel respectively.
    - Models can also be deployed and undeployed using the [Cloud Console](https://console.cloud.google.com/vertex-ai/).
  elements: dict
  returned: success
  type: list
etag:
  description:
    - Used to perform consistent read-modify-write updates.
    - If not set, a blind "overwrite" update happens.
  returned: success
  type: str
modelDeploymentMonitoringJob:
  description:
    - Output only.
    - Resource name of the Model Monitoring job associated with this Endpoint if monitoring is enabled by CreateModelDeploymentMonitoringJob.
    - 'Format: `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`.'
  returned: success
  type: str
state:
  description: The current state of the resource.
  returned: always
  type: str
updateTime:
  description:
    - Output only.
    - Timestamp when this Endpoint was last updated.
  returned: success
  type: str
"""

################################################################################
# Imports
################################################################################

from ansible_collections.google.cloud.plugins.module_utils import gcp_utils as gcp
import types

# BEGIN Custom imports

# END Custom imports


def build_link(module_params, uri):
    params = module_params.copy()

    return ("https://{region}-aiplatform.googleapis.com/v1/" + uri).format(**params)


class DeployedModels(gcp.Resource):
    def _response(self):
        return {
            "automaticResources": DeployedModelsAutomaticResources().from_response(
                self.response.get("automaticResources", {})
            ),
            "createTime": self.response.get("createTime"),
            "dedicatedResources": DeployedModelsDedicatedResources().from_response(
                self.response.get("dedicatedResources", {})
            ),
            "displayName": self.response.get("displayName"),
            "enableAccessLogging": self.response.get("enableAccessLogging"),
            "enableContainerLogging": self.response.get("enableContainerLogging"),
            "id": self.response.get("id"),
            "model": self.response.get("model"),
            "modelVersionId": self.response.get("modelVersionId"),
            "privateEndpoints": DeployedModelsPrivateEndpoints().from_response(
                self.response.get("privateEndpoints", {})
            ),
            "serviceAccount": self.response.get("serviceAccount"),
            "sharedResources": self.response.get("sharedResources"),
        }


class DeployedModelsAutomaticResources(gcp.Resource):
    def _response(self):
        return {
            "maxReplicaCount": self.response.get("maxReplicaCount"),
            "minReplicaCount": self.response.get("minReplicaCount"),
        }


class DeployedModelsDedicatedResources(gcp.Resource):
    def _response(self):
        return {
            "autoscalingMetricSpecs": [
                DeployedModelsDedicatedResourcesAutoscalingMetricSpec().from_response(item)
                for item in (self.response.get("autoscalingMetricSpecs") or [])
            ],
            "machineSpec": DeployedModelsDedicatedResourcesMachineSpec().from_response(
                self.response.get("machineSpec", {})
            ),
            "maxReplicaCount": self.response.get("maxReplicaCount"),
            "minReplicaCount": self.response.get("minReplicaCount"),
        }


class DeployedModelsDedicatedResourcesAutoscalingMetricSpec(gcp.Resource):
    def _response(self):
        return {
            "metricName": self.response.get("metricName"),
            "target": self.response.get("target"),
        }


class DeployedModelsDedicatedResourcesMachineSpec(gcp.Resource):
    def _response(self):
        return {
            "acceleratorCount": self.response.get("acceleratorCount"),
            "acceleratorType": self.response.get("acceleratorType"),
            "machineType": self.response.get("machineType"),
        }


class DeployedModelsPrivateEndpoints(gcp.Resource):
    def _response(self):
        return {
            "explainHttpUri": self.response.get("explainHttpUri"),
            "healthHttpUri": self.response.get("healthHttpUri"),
            "predictHttpUri": self.response.get("predictHttpUri"),
            "serviceAttachment": self.response.get("serviceAttachment"),
        }


class EncryptionSpec(gcp.Resource):
    def _request(self):
        return {
            "kmsKeyName": self.request.get("kms_key_name"),
        }

    def _response(self):
        return {
            "kmsKeyName": self.response.get("kmsKeyName"),
        }


class PredictRequestResponseLoggingConfig(gcp.Resource):
    def _request(self):
        return {
            "bigqueryDestination": gcp.remove_empties(
                PredictRequestResponseLoggingConfigBigqueryDestination(
                    self.request.get("bigquery_destination", {})
                ).to_request()
            ),  # remove empty values
            "enabled": self.request.get("enabled"),
            "samplingRate": self.request.get("sampling_rate"),
        }

    def _response(self):
        return {
            "bigqueryDestination": PredictRequestResponseLoggingConfigBigqueryDestination().from_response(
                self.response.get("bigqueryDestination", {})
            ),
            "enabled": self.response.get("enabled"),
            "samplingRate": self.response.get("samplingRate"),
        }


class PredictRequestResponseLoggingConfigBigqueryDestination(gcp.Resource):
    def _request(self):
        return {
            "outputUri": self.request.get("output_uri"),
        }

    def _response(self):
        return {
            "outputUri": self.response.get("outputUri"),
        }


class PrivateServiceConnectConfig(gcp.Resource):
    def _request(self):
        return {
            "enablePrivateServiceConnect": self.request.get("enable_private_service_connect"),
            "enableSecurePrivateServiceConnect": self.request.get("enable_secure_private_service_connect"),
            "projectAllowlist": self.request.get("project_allowlist"),
            "pscAutomationConfigs": [
                PrivateServiceConnectConfigPscAutomationConfig(item).to_request()
                for item in (self.request.get("psc_automation_configs") or [])
            ],
        }

    def _response(self):
        return {
            "enablePrivateServiceConnect": self.response.get("enablePrivateServiceConnect"),
            "enableSecurePrivateServiceConnect": self.response.get("enableSecurePrivateServiceConnect"),
            "projectAllowlist": self.response.get("projectAllowlist"),
            "pscAutomationConfigs": [
                PrivateServiceConnectConfigPscAutomationConfig().from_response(item)
                for item in (self.response.get("pscAutomationConfigs") or [])
            ],
        }


class PrivateServiceConnectConfigPscAutomationConfig(gcp.Resource):
    def _request(self):
        return {
            "network": self.request.get("network"),
            "projectId": self.request.get("project_id"),
        }

    def _response(self):
        return {
            "errorMessage": self.response.get("errorMessage"),
            "forwardingRule": self.response.get("forwardingRule"),
            "ipAddress": self.response.get("ipAddress"),
            "network": self.response.get("network"),
            "projectId": self.response.get("projectId"),
            "state": self.response.get("state"),
        }


class VertexAI(gcp.Resource):
    def _request(self):
        return {
            "dedicatedEndpointEnabled": self.request.get("dedicated_endpoint_enabled"),
            "description": self.request.get("description"),
            "displayName": self.request.get("display_name"),
            "encryptionSpec": gcp.remove_empties(
                EncryptionSpec(self.request.get("encryption_spec", {})).to_request()
            ),  # remove empty values
            "labels": self.request.get("labels"),
            "network": self.request.get("network"),
            "predictRequestResponseLoggingConfig": gcp.remove_empties(
                PredictRequestResponseLoggingConfig(
                    self.request.get("predict_request_response_logging_config", {})
                ).to_request()
            ),  # remove empty values
            "privateServiceConnectConfig": gcp.remove_empties(
                PrivateServiceConnectConfig(self.request.get("private_service_connect_config", {})).to_request()
            ),  # remove empty values
            "trafficSplit": self.request.get("traffic_split"),
        }

    def _response(self):
        return {
            "createTime": self.response.get("createTime"),
            "dedicatedEndpointDns": self.response.get("dedicatedEndpointDns"),
            "dedicatedEndpointEnabled": self.response.get("dedicatedEndpointEnabled"),
            "deployedModels": [
                DeployedModels().from_response(item) for item in (self.response.get("deployedModels") or [])
            ],
            "description": self.response.get("description"),
            "displayName": self.response.get("displayName"),
            "encryptionSpec": EncryptionSpec().from_response(self.response.get("encryptionSpec", {})),
            "etag": self.response.get("etag"),
            "labels": self.response.get("labels"),
            "modelDeploymentMonitoringJob": self.response.get("modelDeploymentMonitoringJob"),
            "network": self.response.get("network"),
            "predictRequestResponseLoggingConfig": PredictRequestResponseLoggingConfig().from_response(
                self.response.get("predictRequestResponseLoggingConfig", {})
            ),
            "privateServiceConnectConfig": PrivateServiceConnectConfig().from_response(
                self.response.get("privateServiceConnectConfig", {})
            ),
            "trafficSplit": self.response.get("trafficSplit"),
            "updateTime": self.response.get("updateTime"),
        }


################################################################################
# Main
################################################################################


def encode(self, obj):
    """
    This is a function bound to the main resource object. Its input is the object returned from to_request()
    and it mutates it before it is sent to the API.
    """
    return obj


def decode(self, obj):
    """
    This is a function bound to the main resource object. Its input is the object returned from from_response()
    and it mutates it before it is returned to the module caller.
    """
    return obj


def main():
    """Main function"""

    module = gcp.Module(
        argument_spec=dict(
            name=dict(
                type="str",
                required=True,
            ),
            state=dict(
                type="str",
                default="present",
                choices=["present", "absent"],
            ),
            dedicated_endpoint_enabled=dict(
                type="bool",
            ),
            description=dict(
                type="str",
            ),
            display_name=dict(
                type="str",
                required=True,
            ),
            encryption_spec=dict(
                type="dict",
                options=dict(
                    kms_key_name=dict(
                        type="str",
                        required=True,
                        no_log=False,
                    )
                ),
            ),
            labels=dict(
                type="dict",
            ),
            location=dict(
                type="str",
                required=True,
            ),
            network=dict(
                type="str",
            ),
            predict_request_response_logging_config=dict(
                type="dict",
                options=dict(
                    bigquery_destination=dict(
                        type="dict",
                        options=dict(
                            output_uri=dict(
                                type="str",
                            )
                        ),
                    ),
                    enabled=dict(
                        type="bool",
                    ),
                    sampling_rate=dict(
                        type="str",
                    ),
                ),
            ),
            private_service_connect_config=dict(
                type="dict",
                options=dict(
                    enable_private_service_connect=dict(
                        type="bool",
                        required=True,
                    ),
                    enable_secure_private_service_connect=dict(
                        type="bool",
                    ),
                    project_allowlist=dict(
                        type="list",
                        elements="str",
                    ),
                    psc_automation_configs=dict(
                        type="list",
                        elements="dict",
                        options=dict(
                            error_message=dict(
                                type="str",
                            ),
                            forwarding_rule=dict(
                                type="str",
                            ),
                            ip_address=dict(
                                type="str",
                            ),
                            network=dict(
                                type="str",
                                required=True,
                            ),
                            project_id=dict(
                                type="str",
                                required=True,
                            ),
                            state=dict(
                                type="str",
                                choices=["PSC_AUTOMATION_STATE_FAILED", "PSC_AUTOMATION_STATE_SUCCESSFUL"],
                            ),
                        ),
                    ),
                ),
            ),
            region=dict(
                type="str",
            ),
            traffic_split=dict(
                type="str",
            ),
        ),
        mutually_exclusive=[
            ["dedicated_endpoint_enabled", "network", "private_service_connect_config"],
            ["dedicated_endpoint_enabled", "private_service_connect_config"],
            ["network", "private_service_connect_config"],
        ],
    )

    if not module.params["scopes"]:
        module.params["scopes"] = ["https://www.googleapis.com/auth/cloud-platform"]

    state = module.params["state"]
    changed = False
    op_configs = gcp.ResourceOpConfigs(
        {
            "base_url": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{location}/endpoints",
                    "async_uri": "",
                    "verb": "GET",
                    "timeout_minutes": 0,
                }
            ),
            "create": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{location}/endpoints?endpointId={name}",
                    "async_uri": "{op_id}",
                    "verb": "POST",
                    "timeout_minutes": 20,
                }
            ),
            "delete": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{location}/endpoints/{name}",
                    "async_uri": "{op_id}",
                    "verb": "DELETE",
                    "timeout_minutes": 20,
                }
            ),
            "read": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{location}/endpoints/{name}",
                    "async_uri": "",
                    "verb": "GET",
                    "timeout_minutes": 0,
                }
            ),
            "update": gcp.ResourceOpConfig(
                **{
                    "uri": "projects/{project}/locations/{location}/endpoints/{name}",
                    "async_uri": "",
                    "verb": "PATCH",
                    "timeout_minutes": 20,
                }
            ),
        }
    )

    params = gcp.remove_nones(module.params)
    resource = VertexAI(params, module=module, product="VertexAI", kind="vertexai#endpoint")
    read_uri = op_configs.read.uri

    resource._state = state  # store the state in the resource object
    # Bind the encode and decode functions to the resource object
    resource.encode_func = types.MethodType(encode, resource)
    resource.decode_func = types.MethodType(decode, resource)

    custom_diff = None  # Set this variable if you want to implement custom diff logic

    read_url = build_link(params, read_uri)
    existing_obj = resource.get(read_url, allow_not_found=True) or {}
    new_obj = {}
    gcp.debug(module, existing=existing_obj, post=False)

    if custom_diff is not None:
        is_different = custom_diff
    else:
        is_different = resource.diff(gcp.remove_empties(existing_obj))
    gcp.debug(
        module,
        request=gcp.remove_empties(resource.to_request()),
        existing=existing_obj,
        post=True,
        is_different=is_different,
    )

    if gcp.empty(existing_obj):
        if state == "present":
            create_uri = op_configs.create.uri
            create_async_uri = op_configs.create.async_uri
            try:
                # --------- BEGIN create code ---------
                is_async = create_async_uri != ""
                create_link = build_link(params, create_uri)
                create_retries = op_configs.create.timeout
                create_func = getattr(resource, op_configs.create.verb)
                async_create_func = getattr(resource, op_configs.create.verb + "_async")
                async_create_link = build_link(params, "") + create_async_uri
                gcp.debug(
                    module,
                    msg="Creating resource",
                    create_link=create_link,
                    async_create_link=async_create_link,
                    is_async=is_async,
                )

                if is_async:
                    new_obj = async_create_func(create_link, async_link=async_create_link, retries=create_retries)
                else:
                    new_obj = create_func(create_link)
                gcp.debug(module, new=new_obj, action="create", post=False)
                gcp.debug(module, new=new_obj, action="create", post=True)
                # --------- END create code ---------
            except Exception as e:
                module.fail_json(msg=str(e))

            changed = True
        else:
            pass  # nothing to do
    else:
        if state == "absent":
            delete_uri = op_configs.delete.uri
            delete_async_uri = op_configs.delete.async_uri
            try:
                # --------- BEGIN delete code ---------
                is_async = delete_async_uri != ""
                delete_link = build_link(params, delete_uri)
                delete_retries = op_configs.delete.timeout
                delete_func = getattr(resource, op_configs.delete.verb)
                async_delete_func = getattr(resource, op_configs.delete.verb + "_async")
                async_delete_link = build_link(params, "") + delete_async_uri
                gcp.debug(
                    module,
                    msg="Destroying resource",
                    delete_link=delete_link,
                    async_delete_link=async_delete_link,
                    is_async=is_async,
                )
                if is_async:
                    new_obj = async_delete_func(delete_link, async_link=async_delete_link, retries=delete_retries)
                else:
                    new_obj = delete_func(delete_link)
                # --------- END delete code ---------
            except Exception as e:
                module.fail_json(msg=str(e))

            changed = True
        else:
            if is_different:
                update_uri = op_configs.update.uri
                update_async_uri = op_configs.update.async_uri
                try:
                    # --------- BEGIN update code ---------
                    is_async = update_async_uri != ""
                    update_link = build_link(params, update_uri)
                    update_retries = op_configs.update.timeout
                    update_func = getattr(resource, op_configs.update.verb)
                    async_update_func = getattr(resource, op_configs.update.verb + "_async")
                    async_update_link = build_link(params, "") + update_async_uri
                    gcp.debug(
                        module,
                        msg="Updating resource",
                        update_link=update_link,
                        async_update_link=async_update_link,
                        is_async=is_async,
                    )
                    if is_async:
                        new_obj = async_update_func(update_link, async_link=async_update_link, retries=update_retries)
                    else:
                        new_obj = update_func(update_link)
                    gcp.debug(module, new=new_obj, action="update", post=False)
                    gcp.debug(module, new=new_obj, action="update", post=True)
                    # --------- END update code ---------
                except Exception as e:
                    module.fail_json(msg=str(e))

                changed = True
            else:
                new_obj = existing_obj

    new_obj.update({"changed": changed})
    module.exit_json(**new_obj)


if __name__ == "__main__":
    main()
